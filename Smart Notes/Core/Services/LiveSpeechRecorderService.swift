// LiveSpeechRecorderService.swift

import Foundation
import AVFoundation
import Speech
import Combine

class LiveSpeechRecorderService: NSObject, ObservableObject {
    // MARK: - Published properties
    @Published var transcribedText: String = ""
    @Published var isRecording: Bool = false
    @Published var audioLevel: Float = 0.0  // 0.0 ~ 1.0 (waveform ìš©)
    
    // ìµœì¢… ë…¹ìŒ íŒŒì¼ URL (Firebase ì—…ë¡œë“œìš©)
    private(set) var finalRecordingURL: URL?
    
    // MARK: - Private properties
    private let audioEngine = AVAudioEngine()
    private let speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: "en-US"))
    
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    
    private var audioFile: AVAudioFile?   // ì‹¤ì‹œê°„ íŒŒì¼ ì“°ê¸°ìš©
    private let session = AVAudioSession.sharedInstance()
    
    // MARK: - Authorization
    func requestAuthorization() {
        SFSpeechRecognizer.requestAuthorization { status in
            DispatchQueue.main.async {
                switch status {
                case .authorized:
                    print("âœ… Speech recognition authorized")
                default:
                    print("âŒ Speech recognition not authorized: \(status)")
                }
            }
        }
    }
    
    // MARK: - Start Recording + Live STT
    func start() {
        if isRecording { return }
        isRecording = true
        transcribedText = ""
        audioLevel = 0.0
        finalRecordingURL = nil
        
        // 1) Audio Session ì„¤ì •
        do {
            try session.setCategory(.playAndRecord,
                                    mode: .default,
                                    options: [.duckOthers])

            try session.setActive(true, options: .notifyOthersOnDeactivation)
        } catch {
            print("âŒ Audio session setup failed: \(error.localizedDescription)")
        }
        
        // 2) Speech Recognition Request ìƒì„±
        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        recognitionRequest?.shouldReportPartialResults = true
        
        // 3) ì €ì¥í•  íŒŒì¼ URL ìƒì„±
        let filename = UUID().uuidString + ".m4a"
        let url = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask)[0]
            .appendingPathComponent(filename)
        
        // 4) AVAudioEngine input tap ì„¤ì •
        let inputNode = audioEngine.inputNode
        inputNode.removeTap(onBus: 0)
        
        let format = inputNode.outputFormat(forBus: 0)
        
        do {
            // AVAudioFile ìƒì„± (ì‹¤ì‹œê°„ìœ¼ë¡œ bufferë¥¼ ì¨ ë„£ìŒ)
            audioFile = try AVAudioFile(forWriting: url,
                                        settings: format.settings)
            print("ğŸ§ Will record to file:", url)
        } catch {
            print("âŒ Failed to create AVAudioFile:", error.localizedDescription)
        }
        
        inputNode.installTap(onBus: 0,
                             bufferSize: 1024,
                             format: format) { [weak self] buffer, _ in
            guard let self = self else { return }
            
            // â‘  STTìš©ìœ¼ë¡œ buffer append
            self.recognitionRequest?.append(buffer)
            
            // â‘¡ íŒŒì¼ë¡œ ì“°ê¸°
            if let file = self.audioFile {
                do {
                    try file.write(from: buffer)
                } catch {
                    print("âŒ Failed to write buffer to file:", error.localizedDescription)
                }
            }
            
            // â‘¢ audioLevel ê³„ì‚° (waveform)
            self.updateAudioLevel(from: buffer)
        }
        
        // 5) AudioEngine ì‹œì‘
        audioEngine.prepare()
        do {
            try audioEngine.start()
            print("âœ… Audio engine started")
        } catch {
            print("âŒ Audio engine couldn't start:", error.localizedDescription)
        }
        
        // 6) Speech Recognition Task ì‹œì‘
        guard let recognizer = speechRecognizer, let request = recognitionRequest else {
            print("âŒ Speech recognizer or request is nil")
            return
        }
        
        recognitionTask = recognizer.recognitionTask(with: request) { [weak self] result, error in
            guard let self = self else { return }
            
            if let result = result {
                DispatchQueue.main.async {
                    self.transcribedText = result.bestTranscription.formattedString
                }
            }
            
            if let error = error {
                print("âŒ Recognition error:", error.localizedDescription)
                self.stop()
            } else if result?.isFinal == true {
                self.stop()
            }
        }
    }
    
    // MARK: - Stop Recording + STT
    func stop() {
        if !isRecording { return }
        isRecording = false
        
        // tap ì œê±°
        let inputNode = audioEngine.inputNode
        inputNode.removeTap(onBus: 0)
        
        // ì˜¤ë””ì˜¤ ì—”ì§„ ì¢…ë£Œ
        audioEngine.stop()
        audioEngine.reset()
        
        // STT ì •ìƒ ì¢…ë£Œ
        recognitionRequest?.endAudio()
        recognitionTask = nil
        recognitionRequest = nil
        
        // íŒŒì¼ URL ì €ì¥
        if let file = audioFile {
            finalRecordingURL = file.url
            print("âœ… Final recording file URL:", file.url)
        }
        audioFile = nil
        
        // ì„¸ì…˜ ë¹„í™œì„±í™”
        try? session.setActive(false)
    }

    
    // MARK: - Audio Level ê³„ì‚°
    private func updateAudioLevel(from buffer: AVAudioPCMBuffer) {
        guard let channelData = buffer.floatChannelData?[0] else { return }
        let frameLength = Int(buffer.frameLength)
        if frameLength == 0 { return }
        
        // ê°„ë‹¨í•œ RMS ê³„ì‚°
        var sum: Float = 0.0
        for i in 0..<frameLength {
            let sample = channelData[i]
            sum += sample * sample
        }
        let rms = sqrt(sum / Float(frameLength))  // 0 ~ 1 ê·¼ì²˜
        
        // ì ë‹¹íˆ ìŠ¤ì¼€ì¼ë§í•´ì„œ 0~1 í´ë¨í•‘
        let level = min(max(rms * 5, 0.0), 1.0)  // multiplierëŠ” UI ë³´ë©´ì„œ ì¡°ì ˆ
        
        DispatchQueue.main.async {
            self.audioLevel = level
        }
    }
}
